{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Recognition\n",
    "\n",
    "In this notebook, we demonstrare the pipeline of face recognition. The objective is to detect faces on images and describe them using the so called embedding vectors. We will use these vectors to estasblish correspondences between different faces and, potentially, find faces belonging to the same person. This is the exact same principle as the feature matching we saw in Lesson 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection\n",
    "\n",
    "For the sake of completness, we will use the recent YuNet face detector which is included in newer versionf of OpenCV (> 4.7). The YuNet detector is based on convolutional neural networks and the trained weights can be found [here](https://github.com/opencv/opencv_zoo/blob/bdb6f6679d7a215d6c07158615ff4e141482e851/models/face_detection_yunet/face_detection_yunet_2022mar.onnx). Please download them before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize detector\n",
    "detector = cv2.FaceDetectorYN.create(\"face_detection_yunet_2022mar.onnx\", \"\", (320, 320))\n",
    "\n",
    "# Read image\n",
    "source = cv2.imread('data/zelensky.jpg')\n",
    "source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)\n",
    "rows, cols, _ = source.shape\n",
    "\n",
    "# Get image shape\n",
    "img_W = int(source.shape[1])\n",
    "img_H = int(source.shape[0])\n",
    "\n",
    "# Set input size\n",
    "detector.setInputSize((cols, rows))\n",
    "\n",
    "# Run detector\n",
    "faces_src = detector.detect(source)\n",
    "assert faces_src[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the detection matrix corresponds to one face. The format of each row is as follows:\n",
    "\n",
    " * x1, y1, w, h, x_re, y_re, x_le, y_le, x_nt, y_nt, x_rcm, y_rcm, x_lcm, y_lcm\n",
    " \n",
    "where x1, y1, w, h are the top-left coordinates, width and height of the face bounding box, {x, y}_{re, le, nt, rcm, lcm} stands for the coordinates of right eye, left eye, nose tip, the right corner and left corner of the mouth respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show detected faces on image\n",
    "result = np.copy(source)\n",
    "faces_src_img = []\n",
    "\n",
    "for face in faces_src[1]:\n",
    "    # Draw box\n",
    "    x1, y1, w, h = face[0:4]\n",
    "    faces_src_img.append(source[int(y1):int(y1+h), int(x1):int(x1+h), :])\n",
    "    cv2.rectangle(result, (int(x1), int(y1)), (int(x1+w), int(y1+h)), color=(0, 255, 0), thickness=2)    \n",
    "\n",
    "    # Draw landmarks\n",
    "    for idx in range(4, len(face)-1, 2):        \n",
    "        cv2.circle(result, (int(face[idx]), int(face[idx+1])), radius=4, color=(0, 255, 0), thickness=-1)            \n",
    "\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Target Image\n",
    "\n",
    "Let's load a different image on which we want to query the source face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = cv2.imread('data/eu_summit.jpg')\n",
    "target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB)\n",
    "rows, cols, _ = target.shape\n",
    "\n",
    "detector.setInputSize((cols, rows))\n",
    "faces_dst = detector.detect(target)\n",
    "assert faces_dst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = np.copy(target)\n",
    "faces_dst_img = []\n",
    "\n",
    "\n",
    "for face in faces_dst[1]:    \n",
    "    x1, y1, w, h = face[0:4]\n",
    "    faces_dst_img.append(target[int(y1):int(y1+h), int(x1):int(x1+h), :])\n",
    "    cv2.rectangle(result, (int(x1), int(y1)), (int(x1+w), int(y1+h)), color=(0, 255, 0), thickness=2)\n",
    "    \n",
    "    # Draw landmarks\n",
    "    for idx in range(4, len(face)-1, 2):        \n",
    "        cv2.circle(result, (int(face[idx]), int(face[idx+1])), radius=2, color=(0, 255, 0), thickness=-1)            \n",
    "    \n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Recognition\n",
    "\n",
    "Once the face it detected, we need to run the following steps in order to perform a biometric recognition.\n",
    " 1. Face alignment\n",
    " 2. Face embeddings\n",
    " 3. Score evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face Alignment\n",
    "\n",
    "OpenCV face recognition model offers an elegant way to perform face alignment. In order to run it, we need to download the trained model weights from the official repository ([link](https://github.com/opencv/opencv_zoo/tree/master/models/face_recognition_sface))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = cv2.FaceRecognizerSF.create(\"face_recognition_sface_2021dec.onnx\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Align source face\n",
    "faces_src_agn = recognizer.alignCrop(source, faces_src[1][0])\n",
    "\n",
    "plt.subplot(121), plt.imshow(faces_src_img[0])\n",
    "plt.subplot(122), plt.imshow(faces_src_agn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Align target faces\n",
    "\n",
    "faces_dst_agn = [recognizer.alignCrop(target, face) for face in faces_dst[1]]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "for idx, faces in enumerate(zip(faces_dst_img, faces_dst_agn)):\n",
    "    plt.subplot(2,8,idx+1), plt.imshow(faces[0]), plt.axis(False)\n",
    "    plt.subplot(2,8,8+idx+1), plt.imshow(faces[1]), plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face Embeddings\n",
    "\n",
    "We will now use the Sigmoid-Constrained Hypersphere Loss for Robust Face Recognition model to extract the face embeddings. The embeddings are conceptually equivalent to keypoint descriptors seen in Lecture 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "feat_src = recognizer.feature(faces_src_agn)\n",
    "feats_dst = [recognizer.feature(face) for face in faces_dst_agn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Matching\n",
    "\n",
    "After emedding extraction, we need to compute a similarity metric between the source and the candidate faces. There are several options to perform the comparison. The most common metrics are the [L2 norm](https://en.wikipedia.org/wiki/Euclidean_space#Euclidean_norm) and the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for feat_dst in feats_dst:\n",
    "    score_ = recognizer.match(feat_src, feat_dst, cv2.FaceRecognizerSF_FR_COSINE)\n",
    "#     score_ = recognizer.match(feat_src, feat_dst, cv2.FaceRecognizerSF_FR_NORM_L2)\n",
    "\n",
    "    scores.append(score_)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.copy(target)\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "match = faces_dst[1][np.argmin(scores)]\n",
    "x1, y1, w, h = match[0:4]\n",
    "cv2.rectangle(result, (int(x1), int(y1)), (int(x1+w), int(y1+h)), color=(0, 255, 0), thickness=2)\n",
    "    \n",
    "plt.imshow(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
